diff --git a/tensorflow/lite/kernels/internal/max.h b/tensorflow/lite/kernels/internal/max.h
index c1810027..fac3d03d 100644
--- a/tensorflow/lite/kernels/internal/max.h
+++ b/tensorflow/lite/kernels/internal/max.h
@@ -26,7 +26,7 @@ inline float TfLiteMax(const float& x, const float& y) {
 #else
 template <class T>
 inline T TfLiteMax(const T& x, const T& y) {
-  return std::fmax(x, y);
+  return (x > y) ? x : y;
 }
 #endif
 
diff --git a/tensorflow/lite/kernels/internal/min.h b/tensorflow/lite/kernels/internal/min.h
index 62035dcc..c59c75c7 100644
--- a/tensorflow/lite/kernels/internal/min.h
+++ b/tensorflow/lite/kernels/internal/min.h
@@ -26,7 +26,7 @@ inline float TfLiteMin(const float& x, const float& y) {
 #else
 template <class T>
 inline T TfLiteMin(const T& x, const T& y) {
-  return std::fmin(x, y);
+  return (x < y) ? x : y;
 }
 #endif
 
diff --git a/tensorflow/lite/kernels/internal/quantization_util.h b/tensorflow/lite/kernels/internal/quantization_util.h
index 0ee914b0..88f1026a 100644
--- a/tensorflow/lite/kernels/internal/quantization_util.h
+++ b/tensorflow/lite/kernels/internal/quantization_util.h
@@ -133,7 +133,7 @@ IntOut SafeCast(FloatIn x) {
   static_assert(std::numeric_limits<IntOut>::radix == 2, "IntOut is base 2");
 
   // Special case NaN, for which the logic below doesn't work.
-  if (std::isnan(x)) {
+  if (isnan(x)) {
     return 0;
   }
 
@@ -143,7 +143,7 @@ IntOut SafeCast(FloatIn x) {
   }
 
   // Handle infinities.
-  if (std::isinf(x)) {
+  if (isinf(x)) {
     return x < 0 ? std::numeric_limits<IntOut>::min()
                  : std::numeric_limits<IntOut>::max();
   }
diff --git a/tensorflow/lite/micro/kernels/activation_utils.h b/tensorflow/lite/micro/kernels/activation_utils.h
index 95ecc26d..6131f3b8 100644
--- a/tensorflow/lite/micro/kernels/activation_utils.h
+++ b/tensorflow/lite/micro/kernels/activation_utils.h
@@ -28,6 +28,11 @@ namespace tflite {
 namespace ops {
 namespace micro {
 
+inline float signbit(float a)
+{
+  return a <= -0.0f;
+}
+
 // Returns the floating point value for a fused activation:
 inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
   switch (act) {
@@ -42,7 +47,8 @@ inline float ActivationValFloat(TfLiteFusedActivation act, float a) {
     case kTfLiteActTanh:
       return std::tanh(a);
     case kTfLiteActSignBit:
-      return std::signbit(a);
+      // return std::signbit(a);
+      return signbit(a);
     case kTfLiteActSigmoid:
       return 1.0f / (1.0f + std::exp(-a));
   }
diff --git a/tensorflow/lite/micro/kernels/elementwise.cc b/tensorflow/lite/micro/kernels/elementwise.cc
index aa97907d..611d01b3 100644
--- a/tensorflow/lite/micro/kernels/elementwise.cc
+++ b/tensorflow/lite/micro/kernels/elementwise.cc
@@ -75,27 +75,27 @@ inline TfLiteStatus EvalLogical(TfLiteContext* context, TfLiteNode* node,
 }
 
 TfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, std::abs);
+  return EvalNumeric(context, node, fabsf);
 }
 
 TfLiteStatus SinEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, std::sin);
+  return EvalNumeric(context, node, std::sinf);
 }
 
 TfLiteStatus CosEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, std::cos);
+  return EvalNumeric(context, node, std::cosf);
 }
 
 TfLiteStatus LogEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, std::log);
+  return EvalNumeric(context, node, std::logf);
 }
 
 TfLiteStatus SqrtEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, std::sqrt);
+  return EvalNumeric(context, node, std::sqrtf);
 }
 
 TfLiteStatus RsqrtEval(TfLiteContext* context, TfLiteNode* node) {
-  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrt(f); });
+  return EvalNumeric(context, node, [](float f) { return 1.f / std::sqrtf(f); });
 }
 
 TfLiteStatus SquareEval(TfLiteContext* context, TfLiteNode* node) {
